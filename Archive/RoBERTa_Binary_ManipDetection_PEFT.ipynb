{"cells":[{"cell_type":"markdown","id":"dbc10067","metadata":{"id":"dbc10067"},"source":["# RoBERTa-based Manipulation Detection (Binary Classification) ith PEFT/LORA added\n","This notebook uses `roberta-base` to classify dialogue as manipulative or not using the MentalManip dataset."]},{"cell_type":"code","execution_count":1,"id":"6461a70b","metadata":{"executionInfo":{"elapsed":45742,"status":"ok","timestamp":1752450062591,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"6461a70b"},"outputs":[],"source":["!pip install -q transformers\n","!pip install -q datasets\n","!pip install -q evaluate\n","## transformers upgrade\n","!pip install -q --upgrade transformers\n","\n","## Datasets need upgrading to work\n","!pip install -q --upgrade datasets\n","\n","## LoRA addon\n","!pip install -q peft accelerate\n"]},{"cell_type":"code","execution_count":2,"id":"1caed700","metadata":{"executionInfo":{"elapsed":27535,"status":"ok","timestamp":1752450090132,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"1caed700"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.metrics import classification_report\n","\n"]},{"cell_type":"code","source":["model_ckpt = \"roberta-base\""],"metadata":{"id":"kfQpJJa9_E6p","executionInfo":{"status":"ok","timestamp":1752451094439,"user_tz":420,"elapsed":9,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"}}},"id":"kfQpJJa9_E6p","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"id":"a25eddbe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1163,"status":"ok","timestamp":1752451095604,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"a25eddbe","outputId":"06ca0334-a2b0-410b-e1a6-d34bed771c87"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some datasets params were ignored: ['license']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n","WARNING:datasets.load:Some datasets params were ignored: ['license']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'manipulative', 'technique', 'vulnerability'],\n","        num_rows: 4000\n","    })\n","})\n"]}],"source":["# Load the MentalManip dataset (binary classification)\n","# Load dataset\n","dataset = load_dataset(\"audreyeleven/MentalManip\", name=\"mentalmanip_maj\")\n","\n","print(dataset)\n"]},{"cell_type":"code","execution_count":16,"id":"dfe9a948","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1752451095610,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"dfe9a948"},"outputs":[],"source":["# Ensure the 'manipulative' column is class-labeled\n","dataset = dataset.class_encode_column(\"manipulative\")\n","\n","\n"]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","\n","def tokenize_fn(example):\n","    return tokenizer(example[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","tokenized = dataset.map(tokenize_fn, batched=True)\n","print(tokenized)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["356d852fd827494088360cc98c093605","fdd828c4f81c4c47a3a5d388466af736","9fb13e0f881a453da09515449a8fbef2","063914779cc34f42bbc1e2b81ae0398c","be8584e085144cea9269883aef1a88e8","5c9dcfda5c694be4b3841cfe704b98f2","078c181d89a747f193128c84a2c2c974","ed0e8495b4b64665a65e59976cb53cf9","97618c4ec9d049299dc70d24091d900c","72e18456872540ffb48f5133c40151cb","63f3002db1124310b99f3c324fdd3762"]},"id":"DClawgyd16x0","executionInfo":{"status":"ok","timestamp":1752451097960,"user_tz":420,"elapsed":2345,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"}},"outputId":"052a42ad-faec-4d6c-b493-a7963fe329f9"},"id":"DClawgyd16x0","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"356d852fd827494088360cc98c093605"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'manipulative', 'technique', 'vulnerability', 'input_ids', 'attention_mask'],\n","        num_rows: 4000\n","    })\n","})\n"]}]},{"cell_type":"code","source":["## Setup the PEFT addon\n","\n","from peft import get_peft_model, LoraConfig, TaskType\n","\n","# Define your LoRA configuration\n","peft_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,           # Sequence classification\n","    inference_mode=False,                 # True = inference only\n","    r=8,                                  # Low-rank dimension\n","    lora_alpha=16,                        # Scaling factor\n","    lora_dropout=0.1,                     # Dropout in LoRA layers\n","    bias=\"none\",\n","    target_modules=[\"query\", \"value\"]     # Apply only to LoRA layers\n",")\n","\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"Uenf6WH_1W2D","executionInfo":{"status":"ok","timestamp":1752451097974,"user_tz":420,"elapsed":6,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"}}},"id":"Uenf6WH_1W2D","execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"id":"9220dd58","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1752451098161,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"9220dd58","outputId":"72bb6175-29b0-4c62-b7fe-a4a406ef4ce0"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["base_model.model.roberta.encoder.layer.0.attention.self.query\n","base_model.model.roberta.encoder.layer.0.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.0.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.0.attention.self.key\n","base_model.model.roberta.encoder.layer.0.attention.self.value\n","base_model.model.roberta.encoder.layer.0.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.0.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.1.attention.self.query\n","base_model.model.roberta.encoder.layer.1.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.1.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.1.attention.self.key\n","base_model.model.roberta.encoder.layer.1.attention.self.value\n","base_model.model.roberta.encoder.layer.1.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.1.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.2.attention.self.query\n","base_model.model.roberta.encoder.layer.2.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.2.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.2.attention.self.key\n","base_model.model.roberta.encoder.layer.2.attention.self.value\n","base_model.model.roberta.encoder.layer.2.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.2.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.3.attention.self.query\n","base_model.model.roberta.encoder.layer.3.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.3.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.3.attention.self.key\n","base_model.model.roberta.encoder.layer.3.attention.self.value\n","base_model.model.roberta.encoder.layer.3.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.3.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.4.attention.self.query\n","base_model.model.roberta.encoder.layer.4.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.4.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.4.attention.self.key\n","base_model.model.roberta.encoder.layer.4.attention.self.value\n","base_model.model.roberta.encoder.layer.4.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.4.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.5.attention.self.query\n","base_model.model.roberta.encoder.layer.5.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.5.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.5.attention.self.key\n","base_model.model.roberta.encoder.layer.5.attention.self.value\n","base_model.model.roberta.encoder.layer.5.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.5.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.6.attention.self.query\n","base_model.model.roberta.encoder.layer.6.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.6.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.6.attention.self.key\n","base_model.model.roberta.encoder.layer.6.attention.self.value\n","base_model.model.roberta.encoder.layer.6.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.6.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.7.attention.self.query\n","base_model.model.roberta.encoder.layer.7.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.7.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.7.attention.self.key\n","base_model.model.roberta.encoder.layer.7.attention.self.value\n","base_model.model.roberta.encoder.layer.7.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.7.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.8.attention.self.query\n","base_model.model.roberta.encoder.layer.8.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.8.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.8.attention.self.key\n","base_model.model.roberta.encoder.layer.8.attention.self.value\n","base_model.model.roberta.encoder.layer.8.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.8.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.9.attention.self.query\n","base_model.model.roberta.encoder.layer.9.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.9.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.9.attention.self.key\n","base_model.model.roberta.encoder.layer.9.attention.self.value\n","base_model.model.roberta.encoder.layer.9.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.9.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.10.attention.self.query\n","base_model.model.roberta.encoder.layer.10.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.10.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.10.attention.self.key\n","base_model.model.roberta.encoder.layer.10.attention.self.value\n","base_model.model.roberta.encoder.layer.10.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.10.attention.self.value.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.11.attention.self.query\n","base_model.model.roberta.encoder.layer.11.attention.self.query.base_layer\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_dropout\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_dropout.default\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_B.default\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_embedding_A\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_embedding_B\n","base_model.model.roberta.encoder.layer.11.attention.self.query.lora_magnitude_vector\n","base_model.model.roberta.encoder.layer.11.attention.self.key\n","base_model.model.roberta.encoder.layer.11.attention.self.value\n","base_model.model.roberta.encoder.layer.11.attention.self.value.base_layer\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_dropout\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_dropout.default\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_B.default\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_embedding_A\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_embedding_B\n","base_model.model.roberta.encoder.layer.11.attention.self.value.lora_magnitude_vector\n","trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"]}],"source":["\n","base_model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n","\n","# Wrap with LoRA\n","model = get_peft_model(base_model, peft_config)\n","\n","# Pring trainable layers\n","for name, module in model.named_modules():\n","    if \"attention\" in name and any(k in name for k in [\"query\", \"key\", \"value\"]):\n","        print(name)\n","\n","\n","# Optional: Show how many parameters are trainable\n","model.print_trainable_parameters()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":20,"id":"BKbb0_-pJUu6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1752451098214,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"BKbb0_-pJUu6","outputId":"afc1c19a-c939-4b0d-a937-26dd180fbb0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'labels', 'technique', 'vulnerability', 'input_ids', 'attention_mask'],\n","        num_rows: 3200\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'labels', 'technique', 'vulnerability', 'input_ids', 'attention_mask'],\n","        num_rows: 800\n","    })\n","})\n"]}],"source":["# Split the dataset into training and testing sets\n","train_test_split = tokenized[\"train\"].train_test_split(test_size=0.2) # Adjust the test_size as needed\n","\n","# Update the tokenized dataset with the new splits\n","tokenized[\"train\"] = train_test_split[\"train\"]\n","tokenized[\"test\"] = train_test_split[\"test\"]\n","\n","# Make the data work with the nomenclature\n","tokenized = tokenized.rename_column(\"manipulative\", \"labels\")\n","\n","print(tokenized)"]},{"cell_type":"code","execution_count":21,"id":"b952f4a8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1752451098403,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"b952f4a8","outputId":"479db87a-bb16-48bb-9be9-d9a93096dded"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_ckpt,\n","    num_labels=2  # Binary classification\n",")\n"]},{"cell_type":"code","execution_count":22,"id":"60b1378d","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1752451098407,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"60b1378d"},"outputs":[],"source":["## Training args\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./bert-binary-manip\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=8,\n","    logging_dir=\"./logs\",\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    run_name=\"bert-binary-manip\",\n","    report_to=\"none\",\n",")\n"]},{"cell_type":"code","execution_count":23,"id":"r_SG36p2hDTS","metadata":{"executionInfo":{"elapsed":1759,"status":"ok","timestamp":1752451100175,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"r_SG36p2hDTS"},"outputs":[],"source":["## Evaluation metrics\n","\n","import evaluate\n","import numpy as np\n","\n","accuracy = evaluate.load('accuracy')\n","precision = evaluate.load('precision')\n","recall = evaluate.load('recall')\n","f1 = evaluate.load('f1')\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions_argmax = np.argmax(predictions, axis=1)\n","\n","    return {\n","        \"accuracy\": accuracy.compute(predictions=predictions_argmax, references=labels)[\"accuracy\"],\n","        \"precision\": precision.compute(predictions=predictions_argmax, references=labels, average='weighted')[\"precision\"],\n","        \"recall\": recall.compute(predictions=predictions_argmax, references=labels, average='weighted')[\"recall\"],\n","        \"f1\": f1.compute(predictions=predictions_argmax, references=labels, average='weighted')[\"f1\"],\n","    }"]},{"cell_type":"code","execution_count":24,"id":"3be5b395","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"elapsed":1128387,"status":"ok","timestamp":1752452228568,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"3be5b395","outputId":"978c2ee8-2fdd-4184-d32b-56d58e9d840a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-24-3748925629.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1600/1600 18:46, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.616400</td>\n","      <td>0.589591</td>\n","      <td>0.718750</td>\n","      <td>0.681268</td>\n","      <td>0.718750</td>\n","      <td>0.658982</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.642500</td>\n","      <td>0.601798</td>\n","      <td>0.710000</td>\n","      <td>0.504100</td>\n","      <td>0.710000</td>\n","      <td>0.589591</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.569200</td>\n","      <td>0.577618</td>\n","      <td>0.725000</td>\n","      <td>0.694231</td>\n","      <td>0.725000</td>\n","      <td>0.686543</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.588800</td>\n","      <td>0.603179</td>\n","      <td>0.710000</td>\n","      <td>0.504100</td>\n","      <td>0.710000</td>\n","      <td>0.589591</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.508400</td>\n","      <td>0.614099</td>\n","      <td>0.712500</td>\n","      <td>0.673043</td>\n","      <td>0.712500</td>\n","      <td>0.667220</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.528200</td>\n","      <td>0.600194</td>\n","      <td>0.686250</td>\n","      <td>0.679554</td>\n","      <td>0.686250</td>\n","      <td>0.682597</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.529200</td>\n","      <td>0.587600</td>\n","      <td>0.728750</td>\n","      <td>0.700155</td>\n","      <td>0.728750</td>\n","      <td>0.679233</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.369400</td>\n","      <td>0.614563</td>\n","      <td>0.711250</td>\n","      <td>0.676894</td>\n","      <td>0.711250</td>\n","      <td>0.677832</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1600, training_loss=0.5600223599374294, metrics={'train_runtime': 1126.9041, 'train_samples_per_second': 22.717, 'train_steps_per_second': 1.42, 'total_flos': 1683910754304000.0, 'train_loss': 0.5600223599374294, 'epoch': 8.0})"]},"metadata":{},"execution_count":24}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"test\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","trainer.train()\n","\n"]},{"cell_type":"code","execution_count":25,"id":"nmjOPnkmM-Yk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1752452228582,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"nmjOPnkmM-Yk","outputId":"9f04eb7f-02f4-40dc-a7eb-588764a5b845"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Loss: 0.36940\n","Validation Loss: 0.61456\n","Overfitting Ratio: 0.60108\n","No significant overfitting.\n"]}],"source":["\n","##  Easy eval results...\n","##  Get the log history from the trainer state\n","log_history = trainer.state.log_history\n","\n","# Initialize placeholders\n","last_train_loss = None\n","last_eval_loss = None\n","\n","# Iterate through log history to find the last recorded train and eval loss\n","for log in reversed(log_history):\n","    if last_eval_loss is None and \"eval_loss\" in log:\n","        last_eval_loss = log[\"eval_loss\"]\n","    if last_train_loss is None and \"loss\" in log:\n","        last_train_loss = log[\"loss\"]\n","    if last_train_loss is not None and last_eval_loss is not None:\n","        break\n","\n","# Calculate overfitting ratio\n","if last_train_loss is not None and last_eval_loss is not None:\n","    ratio = last_train_loss / last_eval_loss\n","    print(f\"Training Loss: {last_train_loss:.5f}\")\n","    print(f\"Validation Loss: {last_eval_loss:.5f}\")\n","    print(f\"Overfitting Ratio: {ratio:.5f}\")\n","    if ratio < 0.6:\n","        print(\"Overfitting detected!\")\n","    else:\n","        print(\"No significant overfitting.\")\n","else:\n","    print(\"Could not find both training and evaluation loss in log history.\")\n"]},{"cell_type":"code","execution_count":26,"id":"98a675a9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":5938,"status":"ok","timestamp":1752452234522,"user":{"displayName":"Karl-Johan Westhoff","userId":"00382218864597560286"},"user_tz":420},"id":"98a675a9","outputId":"fc9ca312-c5e9-4cd5-dfa8-73308ad5409b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["                  precision    recall  f1-score   support\n","\n","non-manipulative       0.56      0.25      0.35       232\n","    manipulative       0.75      0.92      0.83       568\n","\n","        accuracy                           0.72       800\n","       macro avg       0.65      0.58      0.59       800\n","    weighted avg       0.69      0.72      0.69       800\n","\n"]}],"source":["# Predict on test set\n","preds = trainer.predict(tokenized[\"test\"])\n","y_pred = preds.predictions.argmax(-1)\n","y_true = preds.label_ids\n","\n","# Detailed classification report\n","print(classification_report(y_true, y_pred, target_names=[\"non-manipulative\", \"manipulative\"]))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1N4udwbzS4mbqKTH2uOzDoUICE_72lxyo","timestamp":1752444376839}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"356d852fd827494088360cc98c093605":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdd828c4f81c4c47a3a5d388466af736","IPY_MODEL_9fb13e0f881a453da09515449a8fbef2","IPY_MODEL_063914779cc34f42bbc1e2b81ae0398c"],"layout":"IPY_MODEL_be8584e085144cea9269883aef1a88e8"}},"fdd828c4f81c4c47a3a5d388466af736":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c9dcfda5c694be4b3841cfe704b98f2","placeholder":"​","style":"IPY_MODEL_078c181d89a747f193128c84a2c2c974","value":"Map: 100%"}},"9fb13e0f881a453da09515449a8fbef2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed0e8495b4b64665a65e59976cb53cf9","max":4000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97618c4ec9d049299dc70d24091d900c","value":4000}},"063914779cc34f42bbc1e2b81ae0398c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e18456872540ffb48f5133c40151cb","placeholder":"​","style":"IPY_MODEL_63f3002db1124310b99f3c324fdd3762","value":" 4000/4000 [00:01&lt;00:00, 2413.63 examples/s]"}},"be8584e085144cea9269883aef1a88e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9dcfda5c694be4b3841cfe704b98f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"078c181d89a747f193128c84a2c2c974":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed0e8495b4b64665a65e59976cb53cf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97618c4ec9d049299dc70d24091d900c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72e18456872540ffb48f5133c40151cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63f3002db1124310b99f3c324fdd3762":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}