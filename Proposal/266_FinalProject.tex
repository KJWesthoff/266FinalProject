% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	letterpaper, % Paper size, use either a4paper or letterpaper
	12pt, % Default font size, can also use 11pt or 12pt, although this is not recommended
	unnumberedsections, % Comment to enable section numbering
	twoside, % Two side traditional mode where headers and footers change between odd and even pages, comment this option to make them fixed
]{LTJournalArticle}

\addbibresource{bibliography.bib} % BibLaTeX bibliography file

\runninghead{Manipulative Language Detection in LLM-Crafted Phishing Attacks} % A shortened article title to appear in the running head, leave this command empty for no running head

\footertext{\textit{Final Project} (MICS/DATSCI 266, Summer 2025)} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\usepackage[title,toc,titletoc]{appendix}
\usepackage{titlesec}
\usepackage{lscape}
\usepackage{fontawesome}


\title{Manipulative Language Detection in LLM-Crafted
Phishing Attacks
}  % Article title, use manual lines breaks (\\) to beautify the layout}

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{
	Karl-Johan Westhoff \\
	email: \href{mailto:kjwesthoff@berkeley.edu}{kjwesthoff@berkeley.edu} \\
    Neha Dhage \\
	email: \href{mailto:neha_dhage@ischool.berkeley.edu}{neha\_dhage@ischool.berkeley.edu}
}


% Affiliations are output in the \date{} command
\date{UC Berkeley School of Information \\
MIDS Course 266 Summer 2025 Section 2 (Natalie Ahn) \\
}

% % Full-width abstract
% \renewcommand{\maketitlehookd}{%
% 	\begin{abstract}
% 		\noindent Lorem ipsum dolor sit amet,rta porttitor.
% 	\end{abstract}
% }

%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
\usepackage[title]{appendix}

\begin{document}
\maketitle % Output the title section
%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\section{Introduction}
The human factor remains central in cyber attacks. The 2024 Verizon DBIR report \cite{verizon2024dbir} notes that 68\% of breaches involve the human element, with phishing as a key contributor. With LLM tools, bad actors can now craft highly convincing phishing messages that evade traditional detection.
This project investigates whether NLP models can detect manipulative language—specifically, text designed to influence actions not in the reader's best interest.

Machine learning (ML) models like Naive Bayes and basic neural networks are widely used to filter email traffic for spam (which is an abundant problem). However, they are often limited to detecting specific words or obvious patterns. Newer approaches combine lightweight ML filtering with resource-heavy NLP methods for cases that are not clearly categorized by simpler filtering. Since phishing often exploits human psychology through language, this study focuses on detecting manipulative language and whether such detection may improve defenses against phishing. Although the focus is on cybersecurity, manipulative language also appears in areas such as coercive or abusive communication, highlighting its broader relevance. Our approach first models manipulation using the “Mental Manip” dataset, then explores its potential for phishing detection.

\section{Literature}
Salloum et al. \cite{SALLOUM202119} provide an overview of current ML and NLP methods used for phishing detection, which forms the foundational context for this project.
Suhaima et al. \cite{ImprovedPhishing} trained models like BERT on spam data, whereas our focus will be on specifically detecting manipulative language.
Wang et al. \cite{MentalManip} created a data set aiming at dialogue manipulation, which will serve as our primary training set.
Al-Subaiey et al. have compiled a large corpus of emails in \cite{PhishingEmailDataset} from various datasets, under phishing-specific email body texts; this will be used for attempts to detect phishing texts.

\section{Datasets}
Labeled data sets focused on manipulation are rare. Most of the research has come from psychology, which provides insight into the techniques used for manipulation. Most existing data sets suitable for NLP applications are concerned with hate speech and abusive language, which has been a hot topic in relation to social media.

\section{The MentalManip Dataset}
Wang et al. \cite{MentalManip} introduced the "MentalManip" dataset, published on hugging face \cite{MentalManipDataset}. The data set is based on fictional dialogues from "The Cornell Movie Dialogs Corpus" \cite{CornellMovieCorpus} from which suitable manipulative dialogues were selected using BERT and GPT-4 models, from these 4000 dialogues were manually selected to form the data set. The data is labeled with a detailed manipulation taxonomy in three dimensions; see Figure \ref{fig:Taxonomy}, adding applied technique and psychological vulnerability mechanism to the binary presence of whether the dialogue contains manipulation or not.
\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{Taxonomy.png}
	\caption{Taxonomy labels in the data set}
	\label{fig:Taxonomy}
\end{figure}

The data set was manually labeled using a multi-phase human annotation process, adapting the taxonomy (see Figure \ref{fig:Taxonomy}) to the dialogue context three times by different people annotating. This gave two versions of the data set, one where the majority two out of three constitutes the result ("$MentalManip_{maj}$") and one where all three annotators have consensus and reach the same results ("$MentalManip_{con}$"). The $MentalManip_{maj}$ data set is larger (4000 rows) and more suitable for training a model capturing more instances of manipulation, the $MentalManip_{con}$ data set is smaller (2920 rows) and more precise and better suited for fine tuning. For this project we used the $MentalManip_{maj}$ data. In some cases these data fields are not complete in the data set requiring some degree of feature manipulation, This is addressed in section \ref{sec:FeatureEngineering} below.

\subsection{Data Exploration}\label{sec:DataExploration}

\subsection{Dialogues}
The 4000 dialogues in the data set are between two persons exchanging sentences. By far the majority of dialogues consist of two exchanges, one by each person (there are only three cases with three exchanges). Word count statistics are shown in Figure \ref{fig:DialogueWordCount}, most dialogues consist of up to 50 words per person, and the number of words uttered by each person is fairly balanced, with person 2 saying slightly more words than person 1 in the up to 50 word majority case. Figure \ref{fig:DialogueEmbedding} shows the distribution of token counts for the dialogues in the data set, tokenized using BERT-base as reference. Only a minor number of dialogues exceed the BERT-base embedding size of 512 tokens.


\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{DialogueWordCountStats.png}
	\caption{Word count statistics for the dialogues in the $MentalManip_{maj}$ data set, words uttered by each person}
	\label{fig:DialogueWordCount}
\end{figure}

\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{DialogueEmbeddingStats.png}
	\caption{Statistics for the dialogue in the $MentalManip_{maj}$ data set, tokenized using BERT-base}
	\label{fig:DialogueEmbedding}
\end{figure}


\subsection{Labels}

\subsubsection{Manipulation Label}
The data set is not split equally between manipulation and non-manipulation, Figure \ref{fig:ManipulationRatio} shows the distribution with 2.4 times more manipulation rows than non-manipulation (discussed in section \ref{sec:FeatureEngineering}).

\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{ManipulationRatio.png}
	\caption{Ratio of manipulation to non-manipulation in the $MentalManip_{maj}$ Dataset}
	\label{fig:ManipulationRatio}
\end{figure}

\subsubsection{'Technique' and 'Vulnerability' Labels}
Some of the labels are missing for some of the rows with manipulation present\footnote{The labels should not be populated for non-manipulation rows}, Figure \ref{fig:MissingLabels} shows a total of 664\footnote{110 missing technique and 554 also missing vulnerability} missing labels for 'technique', we regard the technique labels as most relevant for phishing, especially the 'Persuasion or Seduction' label.
\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{MisingLabels.png}
	\caption{Incomplete labeling of the MentalManip Dataset}
	\label{fig:MissingLabels}
\end{figure}

The labels for 'technique' and 'vulnerability' mechanism are not uniformly distributed, furthermore, multiple labels occur in combination as comma separated values see Figure \ref{fig:TechCoOcurrence} showing co-occurrence for 'technique' labels. "Persuasion or Seduction" is the most occurring 'technique' label (this aligns well with use with Phishing).

\begin{figure}[!htp] % Single column :figure	
	\centering
	\includegraphics[width=0.5\textwidth]{TechniqueCoOcurrence.png}
	\caption{Distribution and Co-ocurrence of technique labels}
	\label{fig:TechCoOcurrence}
\end{figure}

Further data exploration can be found in appendix \ref{appendix:DataExploration}




\section{Baselines}
With the relatively short embeddings (see Figure \ref{fig:DialogueEmbedding}), the more basic versions of BERT have sufficient capacity to handle the data. The MentalManip article \cite{MentalManip} also uses some decoder only models by 'zero' and 'few-shot' prompting the model with random example from the data set. This seems to perform better for overall binary classification, but only a little, and the LLM's have a tendency to pick up on toxicity and hate-speech and identify these as manipulation.
Considering the label inconsistencies for 'technique' and 'vulnerability' in the data set, we will focus on the binary classification of manipulation for choosing a baseline model for further experimentation.


\subsection{Binary with BERT and Buddies}
Models looking at the 'manipulative' labels are trained on the $MentalManip_{maj}$ data set. The models were run with similar parameters, and the Accuracy at epoch before significant over fitting\footnote{Significant over fitting defined as: training loss / evaluation loss < 0.6} recorded. Following models were investigated:
\begin{itemize}
	\item BERT-base
	\item RoBERTa
	\item DistilBERT
	\item ModernBERT
\end{itemize}
Furthermore some "emotionally wiser" BERT derivatives exist which are pre-trained for emotion detection:
\begin{itemize}
	\item BERTweet
	\item EmotionBERT
\end{itemize}

\subsubsection{Baseline Results and Discussion}








\section{Feature Engineering}\label{sec:FeatureEngineering}


\begin{itemize}
	\item Address the ratio (e.g. use only the persuasion or seduction labels)
	\item Manipulate the labels - maybe merge the best technique and vulnerability mechanism that fits phishing
	\item Remove rows with missing text data
	\item etc..
\end{itemize}

We address the missing labels (see Figure \ref{fig:MissingLabels}) by either removing the rows with missing labels, or by imputing the missing values with an 'Other' category for the experiments with multi-label inference.




\section{Experiments}
We will build an inference model that can detect manipulated emails based on a deep neural network with transformer architecture.

\section{Evaluation}
Our main interest is to investigate if the model can extend existing phishing detection systems by detecting manipulating language in the emails. We will to look at false negative results from previous models, to see if the detection of manipulative text captures emails that were previously missed.








%----------------------------------------------------------------------------------------
%	 REFERENCES
%----------------------------------------------------------------------------------------
\clearpage
\onecolumn
\printbibliography % Output the bibliography
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	 Appendices
%---------------------------------------------------------------------------------------


\begin{appendices}
	\onecolumn
	%
	\section{Data Exploration}\label{appendix:DataExploration}
	\url{https://drive.google.com/file/d/1s1mIXE58cj8miIoWOiB73VmZaSPs5vc5/view?usp=drive_link}
\end{appendices}
%
\end{document}
